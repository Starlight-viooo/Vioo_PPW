{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18iuD2r3MYOZMhky_1Ojlg51nESZ_TbkU","timestamp":1756350795480}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Web Mining"],"metadata":{"id":"fKUZwp3q0WEi"}},{"cell_type":"markdown","source":["Penambangan web adalah proses menemukan dan mengekstrak informasi dari internet menggunakan berbagai teknik penambangan data. Informasi ini dapat digunakan oleh perusahaan untuk pengambilan keputusan yang efektif\n","\n","\n","berikut ini definisi dari web mining menurut para ahli\n","Penambangan web adalah penggunaan teknik penambangan data untuk secara otomatis menemukan dan mengekstrak informasi dari layanan web\" (Etzioni, 1996; CACM 39).\n","Penambangan web bertujuan untuk menemukan pola-pola berguna atau  pengetahuan dari struktur hiperlink web, isi halaman web, serta perilaku pengguna.\" (Bing Liu, 2007, Web Data Mining).\n"],"metadata":{"id":"vVax28_l0heK"}},{"cell_type":"markdown","source":["Kali ini, saya akan melakukan web crawling/ web ining terhadap web sprynger"],"metadata":{"id":"O8-V8UEZ0tWP"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9-hUzMSSl5L","executionInfo":{"status":"ok","timestamp":1756350843388,"user_tz":-420,"elapsed":11740,"user":{"displayName":"23-167 Vionino Surya Rahmanda","userId":"07764921484846850883"}},"outputId":"25482977-423c-4c33-9bf5-7407693c5d42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sprynger\n","  Downloading sprynger-0.4.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sprynger) (5.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from sprynger) (2.32.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from sprynger) (2.5.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from sprynger) (4.3.8)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (2025.8.3)\n","Downloading sprynger-0.4.1-py3-none-any.whl (40 kB)\n","\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/40.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sprynger\n","Successfully installed sprynger-0.4.1\n"]}],"source":["pip install sprynger"]},{"cell_type":"markdown","source":["## Mendapatkan API Key\n","\n","Buka Portal Developer: Kunjungi situs web Springer Nature API di https://dev.springernature.com/. lalu Daftar Akun anda dengan\n","Klik tombol \"Sign up\" atau \"Register\".\n","Isi formulir pendaftaran dengan nama, email, dan kata sandi Anda.\n","\n","Setelah login, cari menu \"Ours API\" , anda diminta membaca Term Of Condition dan memilih API yang anda cari. di sini saya akan menggunakan Meta API.\n","setelah itu anda masuk ke dasbor Anda. Anda akan melihat Api management anda Di dalam daskboard API management tersebut, Anda akan menemukan API Key yang merupakan serangkaian string panjang random.\n","\n","Salin (copy) API Key ini ke kode python untuk digunakan\n","Kode di bawah ini akan melakukan crawling terhadap semua judul buku yang ada"],"metadata":{"id":"KzZvKw_I3rw7"}},{"cell_type":"markdown","source":["## Proses Web Crawling"],"metadata":{"id":"sdctZLVg5xgM"}},{"cell_type":"markdown","source":["Kode di bawah ini akan melakukan crawling terhadap semua judul buku yang ada"],"metadata":{"id":"nXg3pmvK2Pj9"}},{"cell_type":"code","source":["import requests\n","# Silahkan membuat api key dari https://dev.springernature.com/#api\n","\n","api_key = \"6acc2c2752159536019850ef184ce740\"\n","isbn = \"978-3-031-63497-0\"\n","\n","url = \"https://api.springernature.com/meta/v2/json\"\n","params = {\n","    \"q\": f\"isbn:{isbn}\",\n","    \"api_key\": api_key,\n","    \"p\": 10\n","}\n","\n","response = requests.get(url, params=params)\n","\n","if response.status_code == 200:\n","    data = response.json()\n","    print(f\"Total hasil: {data['result'][0]['total']}\\n\")\n","    for record in data['records']:\n","        doi = record.get('doi', 'N/A')\n","        title = record.get('title', 'No title')\n","        abstract = record.get('abstract', 'No abstract')\n","        print(f\"DOI: {doi}\")\n","        print(f\"Title: {title}\")\n","        print(f\"Abstract: {abstract}\\n\")\n","else:\n","    print(\"Error:\", response.status_code, response.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BplnBquziBA","executionInfo":{"status":"ok","timestamp":1756353497385,"user_tz":-420,"elapsed":1628,"user":{"displayName":"23-167 Vionino Surya Rahmanda","userId":"07764921484846850883"}},"outputId":"15062963-a348-4ddc-eafb-20d0d96b848b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Total hasil: 27\n","\n","DOI: 10.1007/978-3-031-63498-7_20\n","Title: Quantifier Shifting forÂ Quantified Boolean Formulas Revisited\n","Abstract: Modern solvers for quantified Boolean formulas (QBFs) process formulas in prenex form, which divides each QBF into two parts: the quantifier prefix and the propositional matrix. While this representation does not cover the full language of QBF, every non-prenex formula can be transformed to an equivalent formula in prenex form. This transformation offers several degrees of freedom and blurs structural information that might be useful for the solvers. In a case study conducted 20 years back, it has been shown that the applied transformation strategy heavily impacts solving time. We revisit this work and investigate how sensitive recent QBF solvers perform w.r.t. various prenexing strategies.\n","\n","DOI: 10.1007/978-3-031-63498-7_9\n","Title: First-Order Automatic Literal Model Generation\n","Abstract: Given a finite consistent set of ground literals, we present an algorithm that generates a complete first-order logic interpretation, i.e., an interpretation for all ground literals over the signature and not just those in the input set, that is also a model for the input set. The interpretation is represented by first-order linear literals. It can be effectively used to evaluate clauses. A particular application are SCL stuck states. The SCL (Simple Clause Learning) calculus always computes with respect to a finite number of ground literals. It then finds either a contradiction or a stuck state being a model with respect to the considered ground literals. Our algorithm builds a complete literal interpretation out of such a stuck state model that can then be used to evaluate the clause set. If all clauses are satisfied an overall model has been found. If it does not satisfy some clause, this information can be effectively explored to extend the scope of ground literals considered by SCL.\n","\n","DOI: 10.1007/978-3-031-63498-7_3\n","Title: Stepping Stones inÂ theÂ TPTP World\n","Abstract: The TPTP World is a well established infrastructure that supports research, development, and deployment of Automated Theorem Proving (ATP) systems. There are key components that help make the TPTP World a success: the TPTP problem library was first released in 1993, the CADE ATP System Competition (CASC) was conceived after CADE-12 in 1994, problem difficulty ratings were added in 1997, the current TPTP language was adopted in 2003, the SZS ontologies were specified in 2004, the TSTP solution library was built starting around 2005, the Specialist Problem Classes (SPCs) have been used to classify problems since 2010, the SystemOnTPTP service has been offered from 2011, the StarExec service was started in 2013, and a world of TPTP users have helped all along. This paper reviews these stepping stones in the development of the TPTP World.\n","\n","DOI: 10.1007/978-3-031-63498-7_21\n","Title: Satisfiability Modulo Exponential Integer Arithmetic\n","Abstract: SMT solvers use sophisticated techniques for polynomial (linear or non-linear) integer arithmetic. In contrast, non-polynomial integer arithmetic has mostly been neglected so far. However, in the context of program verification, polynomials are often insufficient to capture the behavior of the analyzed system without resorting to approximations. In the last years, incremental linearization has been applied successfully to satisfiability modulo real arithmetic with transcendental functions. We adapt this approach to an extension of polynomial integer arithmetic with exponential functions. Here, the key challenge is to compute suitable lemmas that eliminate the current model from the search space if it violates the semantics of exponentiation. An empirical evaluation of our implementation shows that our approach is highly effective in practice.\n","\n","DOI: 10.1007/978-3-031-63498-7_6\n","Title: Tableaux forÂ Automated Reasoning inÂ Dependently-Typed Higher-Order Logic\n","Abstract: Dependent type theory gives an expressive type system facilitating succinct formalizations of mathematical concepts. In practice, it is mainly used for interactive theorem proving with intensional type theories, with PVS being a notable exception. In this paper, we present native rules for automated reasoning in a dependently-typed version (DHOL) of classical higher-order logic (HOL). DHOL has an extensional type theory with an undecidable type checking problem which contains theorem proving. We implemented the inference rules as well as an automatic type checking mode in Lash, a fork of Satallax, the leading tableaux-based prover for HOL. Our method is sound and complete with respect to provability in DHOL. Completeness is guaranteed by the incorporation of a sound and complete translation from DHOL to HOL recently proposed by Rothgang et al. While this translation can already be used as a preprocessing step to any HOL prover, to achieve better performance, our system directly works in DHOL. Moreover, experimental results show that the DHOL version of Lash can outperform all major HOL provers executed on the translation.\n","\n","DOI: 10.1007/978-3-031-63498-7_22\n","Title: SAT-Based Learning ofÂ Computation Tree Logic\n","Abstract: The CTL learning problem consists in finding for a given sample of positive and negative Kripke structures a distinguishing CTL formula that is verified by the former but not by the latter. Further constraints may bound the size and shape of the desired formula or even ask for its minimality in terms of syntactic size. This synthesis problem is motivated by explanation generation for dissimilar models, e.g. comparing a faulty implementation with the original protocol. We devise a SAT -based encoding for a fixed size CTL formula, then provide an incremental approach that guarantees minimality. We further report on a prototype implementation whose contribution is twofold: first, it allows us to assess the efficiency of various output fragments and optimizations. Secondly, we can experimentally evaluate this tool by randomly mutating Kripke structures or syntactically introducing errors in higher-level models, then learning CTL distinguishing formulas.\n","\n","DOI: 10.1007/978-3-031-63498-7_10\n","Title: Synthesis ofÂ Recursive Programs inÂ Saturation\n","Abstract: We turn saturation-based theorem proving into an automated framework for recursive program synthesis. We introduce magic axioms as valid induction axioms and use them together with answer literals in saturation. We introduce new inference rules for induction in saturation and use answer literals to synthesize recursive functions from these proof steps. Our proof-of-concept implementation in the Vampire theorem prover constructs recursive functions over algebraic data types, while proving inductive properties over these types.\n","\n","DOI: 10.1007/978-3-031-63498-7_23\n","Title: MCSat-Based Finite Field Reasoning inÂ theÂ Yices2 SMT Solver (Short Paper)\n","Abstract: This system description introduces an enhancement to the Yices2 SMT solver, enabling it to reason over non-linear polynomial systems over finite fields. Our reasoning approach fits into the model-constructing satisfiability (MCSat) framework and is based on zero decomposition techniques, which find finite basis explanations for theory conflicts over finite fields. As the MCSat solver within Yices2 can support (and combine) several theories via theory plugins, we implemented our reasoning approach as a new plugin for finite fields and extended Yices2 â€™s frontend to parse finite field problems, making our implementation the first MCSat-based reasoning engine for finite fields. We present its evaluation on finite field benchmarks, comparing it against cvc5 . Additionally, our work leverages the modular architecture of the MCSat solver in Yices2 to provide a foundation for the rapid implementation of further reasoning techniques for this theory.\n","\n","DOI: 10.1007/978-3-031-63498-7_11\n","Title: Synthesizing Strongly Equivalent Logic Programs: Beth Definability forÂ Answer Set Programs viaÂ Craig Interpolation inÂ First-Order Logic\n","Abstract: We show a projective Beth definability theorem for logic programs under the stable model semantics: For given programs P and Q and vocabularyÂ  V (set of predicates) the existence of a programÂ  R in V such that $$P \\cup R$$ P âˆª R and $$P \\cup Q$$ P âˆª Q are strongly equivalent can be expressed as a first-order entailment. Moreover, our result is effective: A programÂ  R can be constructed from a Craig interpolant for this entailment, using a known first-order encoding for testing strong equivalence, which we apply in reverse to extract programs from formulas. As a further perspective, this allows transforming logic programs via transforming their first-order encodings. In a prototypical implementation, the Craig interpolation is performed by first-order provers based on clausal tableaux or resolution calculi. Our work shows how definability and interpolation, which underlie modern logic-based approaches to advanced tasks in knowledge representation, transfer to answer set programming.\n","\n","DOI: 10.1007/978-3-031-63498-7_16\n","Title: Model Completeness forÂ Rational Trees\n","Abstract: We analyze the theory of rational trees with finitely many constructors, infinitely many atoms and an atomicity predicate. We design a new decision procedure, proving in addition that this theory is model-complete. We also show that the enrichment of the language with selectors and simultaneous parametric fixpoints enjoys quantifier elimination.\n","\n"]}]},{"cell_type":"markdown","source":["Kode di bawah ini akan melakukan crawling terhadap buku yang memiliki keyword tertentu, dengan membalik forloop keywords di luar untuk mencari semua keyword\n","\n","lalu kita memasukkan keyword ke dalam parameter respon API webscrapper, berbeda dengan parameter kode sebelum nya yang memasukkan ISBN"],"metadata":{"id":"sfIX_86u2brx"}},{"cell_type":"code","source":["import requests\n","import csv\n","\n","\n","api_key = \"6acc2c2752159536019850ef184ce740\"\n","\n","keywords = ['web mining', 'web usage mining', 'text mining', 'sentiment analysis', 'web structure mining']\n","\n","\n","csv_file_name = \"springer_nature_results.csv\"\n","\n","\n","all_records = []\n","url = \"https://api.springernature.com/meta/v2/json\"\n","\n","print(\"Memulai proses crawling...\")\n","\n","for keyword in keywords:\n","    print(f\"\\n Mencari untuk kata kunci: '{keyword}'\")\n","\n","    params = {\n","        \"q\": f\"keyword:\\\"{keyword}\\\"\", # Mencari berdasarkan kata kunci\n","        \"api_key\": api_key,\n","        \"p\": 10 # Jumlah hasil per halaman (maksimum 100)\n","    }\n","\n","    try:\n","        response = requests.get(url, params=params)\n","        response.raise_for_status()  # Akan error jika status code bukan 2xx\n","\n","        data = response.json()\n","        total_results = int(data['result'][0]['total'])\n","\n","        if total_results > 0:\n","            print(f\"Ditemukan {total_results} hasil. Mengambil {len(data['records'])} data pertama.\")\n","\n","            # Ekstrak informasi dari setiap record\n","            for record in data['records']:\n","                all_records.append({\n","                    'Keyword': keyword,\n","                    'DOI': record.get('doi', 'N/A'),\n","                    'Title': record.get('title', 'No title available'),\n","                    'Abstract': record.get('abstract', '').strip() # Membersihkan spasi ekstra\n","                })\n","        else:\n","            print(\" Tidak ada hasil yang ditemukan.\")\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error saat mengakses API: {e}\")\n","    except KeyError:\n","        print(\"Struktur JSON tidak sesuai, mungkin tidak ada record yang ditemukan.\")\n","\n","# --- MENYIMPAN KE CSV ---\n","if all_records:\n","    print(f\"\\n Menyimpan total {len(all_records)} data ke file '{csv_file_name}'...\")\n","\n","    try:\n","        with open(csv_file_name, mode='w', newline='', encoding='utf-8') as csv_file:\n","            # Tentukan header untuk file CSV\n","            fieldnames = ['Keyword', 'DOI', 'Title', 'Abstract']\n","            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","\n","            writer.writeheader() # Menulis header\n","            for record in all_records:\n","                writer.writerow(record) # Menulis setiap baris data\n","\n","        print(f\" Proses selesai! File '{csv_file_name}' berhasil dibuat.\")\n","    except IOError as e:\n","        print(f\"Gagal menulis file CSV: {e}\")\n","else:\n","    print(\"\\nTidak ada data untuk disimpan.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9khRC6fSxGH","executionInfo":{"status":"ok","timestamp":1756351619809,"user_tz":-420,"elapsed":13466,"user":{"displayName":"23-167 Vionino Surya Rahmanda","userId":"07764921484846850883"}},"outputId":"6e859eee-f2d2-4e84-ace6-e59e19cb88f2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Memulai proses crawling...\n","\n","ðŸ”Ž Mencari untuk kata kunci: 'web mining'\n","âœ… Ditemukan 437 hasil. Mengambil 10 data pertama.\n","\n","ðŸ”Ž Mencari untuk kata kunci: 'web usage mining'\n","âœ… Ditemukan 198 hasil. Mengambil 10 data pertama.\n","\n","ðŸ”Ž Mencari untuk kata kunci: 'text mining'\n","âœ… Ditemukan 3593 hasil. Mengambil 10 data pertama.\n","\n","ðŸ”Ž Mencari untuk kata kunci: 'sentiment analysis'\n","âœ… Ditemukan 6591 hasil. Mengambil 10 data pertama.\n","\n","ðŸ”Ž Mencari untuk kata kunci: 'web structure mining'\n","âœ… Ditemukan 20 hasil. Mengambil 10 data pertama.\n","\n","ðŸ’¾ Menyimpan total 50 data ke file 'springer_nature_results.csv'...\n","ðŸš€ Proses selesai! File 'springer_nature_results.csv' berhasil dibuat.\n"]}]}]}